\documentclass{homework}
\author{Morales, Samantha}
\class{CSCI 2114: Tashfeen's Data Structures}
\date{\today}
\title{Homework 2}
\address{%
  Oklahoma City University, %
  Petree College of Arts \& Sciences, %
  Computer Science%
}

\acmfonts

\begin{document} \maketitle

Let's remind ourselves of the asymptotic order
notation\footnote{Sometimes also referred to as the
  \href{https://en.wikipedia.org/wiki/Big_O_notation}{Bachmann-Landau
    notation}.}. Let $f(x)$ and $g(x)$ be functions of a positive $x$.
\[
  f(x) = \O(g(x))
\]
when there is positive constant $c$ such that,
\[
  f(x) \leq cg(x)
\]
for all $x \geq n$. Function $f$ is then stated as \textit{big oh}
of $g$. Similarly, $f$ is \textit{big omega} of $g$ \ie
\[
  f(x) = \Omega(g(x))
\]
when there exist positive constants $c, n$ such that for all $x
  \geq n$ we have,
\[
  f(x) \geq cg(x)
\]
Lastly, if $f$ is both big-$\O$ and big-$\Omega$ of $g$,
\[
  \Omega(g(x)) = f(x) = \O(g(x))
\]
Then such a tight bound is stated as $f$ is big-$\Theta$ of $g$,
\[
  f(x) = \Theta(g(x))
\]

\question Consider the following Java subroutine,
\begin{lstlisting}[language=java]
public static void conditionalWork(int n) {
    for (int i = 0; i < n; i++)
        if (Math.random() < 0.5)
            taskA()
        else
            taskB()
}


\end{lstlisting}
\begin{enumerate}
  \item If on a certain machine the function \texttt{taskA()} takes 2
        seconds on each call in the loop and the function \texttt{taskB()}
        takes 4 seconds then for $n=10$, what is the total number of
        \textit{expected} seconds taken by the subroutine \texttt{public
          static void conditionalWork(int n)}?

        % solution goes here
        \begin{sol}
          % (5 * 2) + (4 * 5)
          % 10 + 20 = 30
          The subroutine conditionalWork is expected to take 30 seconds to run.
        \end{sol}
        

  \item The subroutine \texttt{conditionalWork(int n)} is ran on a much
        faster machine reducing the time taken by \texttt{taskA()} on each
        call down to $\frac{1}{25}^\text{th}$ of a second and
        \texttt{taskB()} now takes $\frac{1}{5}^\text{th}$ of a second.
        For ${n=10}$, what is the new total number of seconds
        \textit{expected} by the subroutine \texttt{conditionalWork(int
          n)}?

        % solution goes here
        \begin{sol}
          % (5/25) + (1) = 1.2
          % 5/25 = 0.2
          The subroutine conditionalWork would take 1.2 seconds on the faster machine.
        \end{sol}
        

  \item Assume that for any arbitrary $n$, \texttt{taskA()} takes
        $\log(n)$ steps while \texttt{taskB()} takes $2n^2$ steps. Let
        $T(n)$ be the total number of steps \textit{expected} by
        \texttt{conditionalWork(int n)}, what is $T(n)$?

        % solution goes here
        \begin{sol}
          % (0.5n log(n)) + (0.5n 2n^2)
          % (0.5n log(n)) + (n^3)
          $T(n)$ is $\frac{n}{2}\log(n) + n^3$.
        \end{sol}

  \item What is the \textit{best expected asymptotic} runtime complexity
        written as $T(n) = \Omega(g(n))$?

        % solution goes here
        \begin{sol}
          $T(n) = \Omega(g(n)) = n\log(n)$
        \end{sol}

  \item What is the \textit{worst expected asymptotic} runtime complexity
        written as $T(n) = \O(g(n))$?

        % solution goes here
        \begin{sol}
          $T(n) = \O(g(n)) = n^3$
        \end{sol}
  \item If \texttt{taskB()} took $4\log(n)$ steps, what is the
        \textit{tight expected asymptotic} runtime complexity written as
        $T(n) = \Theta(g(n))$?

        % solution goes here
        \begin{sol}
          % log(n) + log(n) = 2log(n)
          % 2log(n) * 0.5n = nlog(n)
          $T(n) = \Theta(g(n)) = n\log(n)$
        \end{sol}
\end{enumerate}

\question In the code snippet bellow, we see two ways of getting the tenth
place digit of a Java \texttt{int} $n$. Give the runtime
complexity of each method using the \textit{big oh} notation.
Justify your answer.
\begin{lstlisting}[language=java]
System.out.println(n % 10);
System.out.println(String.valueOf(n).charAt(String.valueOf(n).length() - 1));
\end{lstlisting}

% solution goes here
\begin{sol}
  Both methods have a runtime complexity of $\O(g(n)) = n$.
  This is because both methods simply take in an argument n and return the 
  tenth place digit in one step.
\end{sol}

\question Consider the following functions,
\begin{align*}
  a(x)      & = 2(x^3+1)(x^2-1)+2 \\
  b(x)      & = 2x^2              \\
  \alpha(x) & = x^2               \\
  \beta(x)  & = \pi x^2
\end{align*}
\begin{enumerate}
  \item Observe that $b(x) = \O(a(x))$ because $b(x) \leq a(x)$ past a
        certain $x=n$. What is the value of $n$ in this case?

        % solution goes here
        \begin{sol}
          For $b(x)$ is $\leq a(x)$ for all $n \geq 1$
        \end{sol}

  \item Observe that $\beta(x) = \O(\alpha(x))$ because there exists a $c$
        such that $\beta(x) \leq c\alpha(x)$ for all $x > 0$. What is the
        value of $c$ in this case?

        % solution goes here
        \begin{sol}
          The statement $\beta(x) \leq c\alpha(x)$ is true for all values of $c \geq \pi$ 
        \end{sol}


\end{enumerate}

\question Given in listing 1 is a Java subroutine that sorts an
array of non-negative Java integers in ascending order.

%   \lstinputlisting[
%     language={java},
%     linerange={10-20},
%     caption={Linear time algorithm for sorting unique integers.},
%     label=sort]
% {./code/Counting.java}

Assume that the length of the input parameter \texttt{int[]
  toSort} is $n + 1$, the ${\max(\texttt{toSort}) \leq n}$ and that
\texttt{toSort} has unique elements. Let $T(n)$ be the worst and
the average case complexity of the algorithm's runtime and
$S(\texttt{toSort})$ be the worst case complexity of the
memory-space.

\begin{enumerate}
  \item What is $T(n)$?

        % solution goes here
        \begin{sol}
          Each for loop gives us a time complexity of $(n + 1)$ so for 3 loops
          that gives us $(3n + 3)$. If we then add each individual process we must add
          $+ 1$ for every variable declaration totaling 5. Hence our answer is $T(n) = 3n + 8$
          
        \end{sol}


  \item What are $\O(T(n)), \O(S(\texttt{toSort}))$?

        % solution goes here
        \begin{sol}
          $\O(T(n)) = n$ 

          $\O(S(\texttt{toSort})) = n$
        \end{sol}

  \item Look up and state the \textit{big}-$\O$ of the average case
        complexity of Java's built-in
        \href{https://tinyurl.com/jw75mfa}{\texttt{Arrays.sort(int[])}}.
        Is this better than $\O(T(n))$ from the previous step of this question?

        % solution goes here
        \begin{sol}
          The average case complexity for Java's \texttt{Arrays.sort(int[])} is 
          $\O(n\log(n))$. This is generally considered to be worse than $\O(n)$ because 
          for $n \geq 10$ linear time is the same or faster.
        \end{sol}
\end{enumerate}

\question Given bellow is the Java implementation of the sieve of
Eratosthenes. This particular implementation marks all the
composite numbers as \texttt{true} since Java allocates
\texttt{false} to all the elements of a newly created boolean
arrays.
\begin{lstlisting}[language=java]
public static void eratosthenes(boolean[] toSieve) {
    toSieve[0] = true;
    toSieve[1] = true;
    for (int i = 2; i < Math.sqrt(toSieve.length); i++)
        if (!toSieve[i])
            for (int j = i*i; j < toSieve.length; j += i)
                toSieve[j] = true;
}
\end{lstlisting}
Give an upper-bound on the runtime complexity of this particular
implementation of the sieve of Eratosthenes. The better
upper-bound you give, the more credit you get. Justify your
answer.

% solution goes here
\begin{sol}
% \sqrt(n) * (n*freq of squares)
The outer loop repeats $\sqrt{n} - 2$ times while the inner loop 
repeats once for every multiple less than n for the range $(2 \leq i \leq \sqrt{n})$.
Basically, we're doing a cumulative sum we can approximate this as $ln(\sqrt{n})$ which we can simplify to 
$\frac{1}{2}\ln(n)$. Since we know that we are doing this sum for every single prime number, meaning that $i$ will 
never be composite, we can multiply our cumulative sum approximation by $0.48x$ (0.48 being the approximate number 
of primes between 2 and $2^{31}-1$). We can simplify one last time by multiplying $0.48x$ by the $\frac{1}{2}$ before the ln. This gives us the final upper-bound of... \bigbreak

$0.24n\ln(n)$ \bigbreak

which can be written as $\O(n\ln(n))$.
\end{sol}
  
\section*{Submission Instructions}
Submit a PDF file with your answers.
\end{document}

